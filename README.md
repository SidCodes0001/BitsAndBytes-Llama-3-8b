# BitsAndBytes-Llama-3-8b

In this practise session I have applied 4-bit quantization to the LLaMA 3 model.

## Notes:
- Quantization: This is a process where you convert the high-precision numbers to lower precision.
- When the model is running (making predictions), it uses a more precise format (bfloat16) to do the math.
